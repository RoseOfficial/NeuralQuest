# NeuralQuest Maximum Exploration Configuration for Pokemon Red
# Designed for discovering as much of the game world as possible

[env]
frame_skip = 4
sticky_p = 0.01         # Minimal sticky actions for maximum exploration variety
seed = 1337
max_episode_steps = 100000  # Very long episodes for deep exploration
deterministic = true

[algo]
gamma = 0.9999          # Very high discount for extreme long-term thinking
gae_lambda = 0.98       # High GAE for long sequences
lr_policy = 1e-4        # Conservative learning rate
lr_value = 1e-4
batch_horizon = 8192    # Large batches for stable learning
entropy_coeff = 0.1     # Maximum entropy for exploration
value_coeff = 0.3       # Lower value weight to emphasize policy exploration
grad_clip = 15.0        # Higher clipping for stability
epochs_per_update = 6   # More training per batch
minibatch_size = 1024   # Large minibatches

[rnd]
beta = 0.5              # Maximum intrinsic motivation
lr = 5e-4               # Moderate RND learning
reward_clip = 20.0      # High reward ceiling for diverse discoveries
norm_ema = 0.999        # Very slow normalization for exploration consistency
hidden_dim = 512        # Large networks for complex pattern recognition

[archive]
hash_bits = 64
capacity = 200000       # Massive archive for comprehensive state coverage
p_frontier = 0.6        # High frontier sampling for exploration focus
novel_lru = 20000       # Large LRU for long-term novelty tracking
evict_on = "old_and_often"
hamming_threshold = 1   # Very sensitive novelty detection
projection_dim = 256    # Large projection for fine-grained state distinction

[train]
epochs = 100000         # Extended training for thorough exploration
ckpt_every = 1000       # Regular checkpoints for long training
log_every = 100         # Detailed logging
eval_every = 5000       # Periodic evaluation
save_dir = "pokemon_exploration_checkpoints"
log_dir = "pokemon_exploration_logs"