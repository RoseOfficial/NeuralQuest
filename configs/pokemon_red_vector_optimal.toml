# NeuralQuest Configuration for Pokemon Red - OPTIMAL VECTORIZED TRAINING
# Optimized settings for fast, efficient training with 2048 batch horizon

[env]
frame_skip = 4          # Standard frame skip for RPG pacing
sticky_p = 0.1          # Higher sticky for more persistent actions
seed = 1337             # Reproducible seed
max_episode_steps = 2000 # Shorter episodes to reduce memory pressure
deterministic = true    # Enable deterministic execution
headless = true         # Default headless (overridden by visual_env_idx)
use_progress_detector = false  # Disable progress detector for maximum performance

[algo]
gamma = 0.995           # Slightly lower discount to focus on shorter-term exploration
gae_lambda = 0.95       # Standard GAE lambda
lr_policy = 1e-4        # Stable learning rate for policy updates
lr_value = 1e-4         # Matched value learning rate
batch_horizon = 2048    # OPTIMAL: Fast updates while maintaining stability (204 steps per env)
entropy_coeff = 0.05    # Moderate entropy to prevent premature convergence
value_coeff = 0.5       # Standard value coefficient
grad_clip = 2.0         # Conservative gradient clip for stability
epochs_per_update = 2   # Fewer epochs per update to reduce memory
minibatch_size = 512    # Smaller minibatches for memory efficiency

[rnd]
beta = 0.3              # Moderate intrinsic reward
lr = 1e-4               # Stable RND learning rate
reward_clip = 5.0       # Conservative clip
norm_ema = 0.999        # Slower normalization to prevent variance collapse
hidden_dim = 256        # Standard RND network size

[archive]
hash_bits = 64          # Full 64-bit hashing
capacity = 500000       # Large archive for comprehensive exploration
p_frontier = 0.6        # High frontier probability for exploration
novel_lru = 50000       # Large LRU window for novelty detection
evict_on = "old_and_often"  # Intelligent eviction strategy
hamming_threshold = 2   # Fine-grained novelty detection
projection_dim = 64     # Efficient projection dimension

[train]
epochs = 10000          # Long training for comprehensive learning
ckpt_every = 1          # Save every epoch for progress tracking
log_every = 5           # More frequent logging for better monitoring
eval_every = 50         # Regular evaluation for performance tracking
save_dir = "pokemon_vector_checkpoints_optimal"
log_dir = "pokemon_vector_logs_optimal"
n_envs = 10             # 10 parallel PyBoy instances
visual_env_idx = -1     # All headless for maximum performance