# NeuralQuest Configuration for Pokemon Red - Progress-Focused
# Enhanced with learned progress detection for goal-directed exploration
# Maintains domain-agnostic principles while learning what constitutes progress

[env]
frame_skip = 4          # Standard frame skip for RPG pacing
sticky_p = 0.05         # Lower sticky probability for precise menu navigation
seed = 1337             # Reproducible seed
max_episode_steps = 2000   # Slightly longer episodes for deeper exploration
deterministic = true    # Enable deterministic execution

[algo]
gamma = 0.999           # Higher discount for long-term planning in RPGs
gae_lambda = 0.97       # Higher GAE lambda for temporal credit assignment
lr_policy = 1.5e-4      # Slightly lower learning rate for stability with progress rewards
lr_value = 1.5e-4       # Matched value learning rate
batch_horizon = 2000    # Match episode length for proper episode completion
entropy_coeff = 0.015   # Slightly lower entropy for more directed exploration
value_coeff = 0.5       # Standard value coefficient
grad_clip = 10.0        # Higher gradient clip for stability
epochs_per_update = 4   # Multiple epochs per update
minibatch_size = 512    # Larger minibatches

[rnd]
beta = 0.1              # Lower intrinsic reward scaling (progress rewards supplement)
lr = 6e-4               # Slightly lower RND learning rate
reward_clip = 8.0       # Lower reward clip since progress provides additional signal
norm_ema = 0.995        # Slower normalization for stable long-term training
hidden_dim = 256        # Larger RND networks for complex state space

[archive]
hash_bits = 64          # Full 64-bit hashing for large state space
capacity = 100000       # Much larger archive for RPG world exploration
p_frontier = 0.85       # Higher frontier probability for progress-directed sampling
novel_lru = 10000       # Larger LRU window for RPG state persistence
evict_on = "old_and_often"  # Eviction strategy
hamming_threshold = 3   # Higher threshold for meaningful state differences
projection_dim = 128    # Larger projection for rich state representation

[progress]
# Progress detector configuration
hidden_dim = 256        # Hidden dimension for progress networks
learning_rate = 3e-4    # Learning rate for progress detector
discount = 0.99         # Discount factor for temporal difference learning
update_freq = 100       # Update progress detector every N steps

[train]
epochs = 50000          # Very long training for RPG mastery
ckpt_every = 500        # More frequent checkpoints for long training
log_every = 50          # Less frequent logging
eval_every = 2500       # Periodic evaluation
save_dir = "pokemon_progress_checkpoints"  # Progress-specific checkpoint directory
log_dir = "pokemon_progress_logs"          # Progress-specific log directory